<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="MFIN7036 Students 2025" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Group Human Superintelligence, Progress Report, " />

<meta property="og:title" content="Data Collection: Perform Web Scraping on New York Times and Reddit (by Group &#34;Human Superintelligence&#34;) "/>
<meta property="og:url" content="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/data-collection-perform-web-scraping-on-new-york-times-and-reddit-by-group-human-superintelligence.html" />
<meta property="og:description" content="By Group &#34;Human Superintelligence&#34; Abstract In recent years, the fluctuations in the gold price have always been the focus of numerous investors and market analysts. The core objective of our research is to deeply explore the correlation between gold price fluctuations and market sentiment. This research on the correlation not …" />
<meta property="og:site_name" content="MFIN7036 Student Blog 2025" />
<meta property="og:article:author" content="MFIN7036 Students 2025" />
<meta property="og:article:published_time" content="2025-01-17T01:12:00+08:00" />
<meta name="twitter:title" content="Data Collection: Perform Web Scraping on New York Times and Reddit (by Group &#34;Human Superintelligence&#34;) ">
<meta name="twitter:description" content="By Group &#34;Human Superintelligence&#34; Abstract In recent years, the fluctuations in the gold price have always been the focus of numerous investors and market analysts. The core objective of our research is to deeply explore the correlation between gold price fluctuations and market sentiment. This research on the correlation not …">

        <title>Data Collection: Perform Web Scraping on New York Times and Reddit (by Group &#34;Human Superintelligence&#34;)  · MFIN7036 Student Blog 2025
</title>
        <link href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="MFIN7036 Student Blog 2025 - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/"><span class=site-name>MFIN7036 Student Blog 2025</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://buehlmaier.github.io/MFIN7036-student-blog-2025-01
                                    >Home</a>
                                </li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/categories.html">Categories</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/tags.html">Tags</a></li>
                                <li ><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/data-collection-perform-web-scraping-on-new-york-times-and-reddit-by-group-human-superintelligence.html">
                Data Collection: Perform Web Scraping on New York Times and Reddit (by Group "Human Superintelligence")
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <p>By Group "Human Superintelligence"</p>
<h2>Abstract</h2>
<p>In recent years, the fluctuations in the gold price have always been the focus of numerous investors and market analysts. The core objective of our research is to deeply explore the correlation between gold price fluctuations and market sentiment. This research on the correlation not only helps investors better understand the mechasim of gold price formation, but also provides a unique perspective and valuable data support for macroeconomic research.</p>
<p>During the research process, data collection is of utmost importance. This blog showcases the entire process of using Python for web crawling and downloading text data. In order to obtain comprehensive and representative data, we selected The New York Times and the Reddit forum as the sources of text data. As a globally renowned media, The New York Times has in-depth and extensive coverage of various economic events and market dynamics. The Reddit forum, on the other hand, gathers users from all over the world. They share their views and discussions on a wide range of topics on the forum, including the gold market.</p>
<h2>Web Scraping on The New York Times</h2>
<h3>1. Importing Required Libraries and Setting Up the NYT API</h3>
<p>We started by importing the necessary libraries to support the script's functionality: <strong>requests</strong> for making API calls, <strong>pandas</strong> for managing and organizing data, <strong>datetime</strong> for parsing and formatting dates, and <strong>newspaper</strong> for scraping and extracting the content of news articles. Once the tools were in place, we configured the NYT API by setting up the API key, endpoint, and query parameters. These parameters included a search keyword <strong>"gold price"</strong>, a specified date range, and a sorting preference to ensure the fetched news articles were relevant and focused. This setup allowed us to efficiently retrieve gold-related news articles from The New York Times API, laying the groundwork for further data processing and analysis.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">newspaper</span> <span class="kn">import</span> <span class="n">Article</span>

<span class="c1"># Replace with your New York Times API key</span>
<span class="n">API_KEY</span> <span class="o">=</span> <span class="s2">&quot;your API_KEY&quot;</span>

<span class="c1"># Set API endpoint and parameters</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;https://api.nytimes.com/svc/search/v2/articlesearch.json&quot;</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;api-key&quot;</span><span class="p">:</span> <span class="n">API_KEY</span><span class="p">,</span>
    <span class="s2">&quot;q&quot;</span><span class="p">:</span> <span class="s2">&quot;gold price&quot;</span><span class="p">,</span>  <span class="c1"># Search keyword</span>
    <span class="s2">&quot;begin_date&quot;</span><span class="p">:</span> <span class="s2">&quot;statr_date&quot;</span><span class="p">,</span>  <span class="c1"># Start date</span>
    <span class="s2">&quot;end_date&quot;</span><span class="p">:</span> <span class="s2">&quot;end_date&quot;</span><span class="p">,</span>  <span class="c1"># End date</span>
    <span class="s2">&quot;sort&quot;</span><span class="p">:</span> <span class="s2">&quot;relevance&quot;</span>  <span class="c1"># Sort by relevance</span>
<span class="p">}</span>
</code></pre></div>

<h3>2. Crawling and Saving Gold Matket News from the API</h3>
<p>We sent a <strong>GET request</strong> to the NYT API to fetch gold-related news articles. If the request was successful, we parsed the <strong>JSON response</strong> into a Python dictionary, allowing us to easily access and manipulate the data for further processing and analysis. This step was crucial for extracting structured information from the API response and preparing it for storage and analysis.</p>
<div class="highlight"><pre><span></span><code> <span class="c1"># Send GET request</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">base_url</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
    <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>  <span class="c1"># Check if the request was successful</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>
</code></pre></div>

<p>We designed the script to iterate through the list of articles returned in the API response. For each article, we extracted the publication date and formatted it into a more readable <strong>YYYY-MM-DD</strong> format. This step ensured that the dates were standardized and easier to work with during subsequent data analysis and storage.</p>
<div class="highlight"><pre><span></span><code> <span class="c1"># Parse the returned data</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;response&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;docs&quot;</span><span class="p">,</span> <span class="p">[]):</span>
        <span class="c1"># Handle datetime format</span>
        <span class="n">pub_date</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pub_date&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Attempt to parse datetime</span>
            <span class="n">parsed_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">pub_date</span><span class="p">,</span> <span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">T%H:%M:%S+0000&quot;</span><span class="p">)</span>
            <span class="n">formatted_date</span> <span class="o">=</span> <span class="n">parsed_date</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="c1"># If parsing fails, use a default value or skip</span>
            <span class="n">formatted_date</span> <span class="o">=</span> <span class="s2">&quot;Unknown date&quot;</span>
</code></pre></div>

<p>We utilized the newspaper <strong>library</strong> to download and parse the full text of each article from its URL. This allowed us to extract the complete content of the articles for deeper analysis. If the download or parsing failed due to issues like invalid URLs or network errors, we implemented a fallback mechanism that used a placeholder message ("Failed to retrieve content") to ensure the script continued running smoothly without interruption.</p>
<div class="highlight"><pre><span></span><code> <span class="c1"># Get article content</span>
        <span class="n">article_url</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;web_url&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Use the newspaper library to fetch the article</span>
            <span class="n">article</span> <span class="o">=</span> <span class="n">Article</span><span class="p">(</span><span class="n">article_url</span><span class="p">)</span>
            <span class="n">article</span><span class="o">.</span><span class="n">download</span><span class="p">()</span>
            <span class="n">article</span><span class="o">.</span><span class="n">parse</span><span class="p">()</span>
            <span class="n">article_content</span> <span class="o">=</span> <span class="n">article</span><span class="o">.</span><span class="n">text</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">article_content</span> <span class="o">=</span> <span class="s2">&quot;Failed to retrieve content&quot;</span>
</code></pre></div>

<p>Then we organized the key details of each article—such as the title, abstract, publication date, URL, and content—into a structured dictionary. This dictionary was then appended to the news_data list, creating a centralized and organized collection of all the extracted article information. This approach allowed us to efficiently manage and prepare the data for further processing, such as saving it to a CSV file or performing analysis.</p>
<div class="highlight"><pre><span></span><code><span class="n">article_data</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;headline&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">,</span> <span class="s2">&quot;No title&quot;</span><span class="p">),</span>  <span class="c1"># Title</span>
            <span class="s2">&quot;abstract&quot;</span><span class="p">:</span> <span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;abstract&quot;</span><span class="p">,</span> <span class="s2">&quot;No abstract&quot;</span><span class="p">),</span>  <span class="c1"># Abstract</span>
            <span class="s2">&quot;published_date&quot;</span><span class="p">:</span> <span class="n">formatted_date</span><span class="p">,</span>  <span class="c1"># Publication date</span>
            <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="n">article_url</span><span class="p">,</span>  <span class="c1"># URL</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">article_content</span>  <span class="c1"># Article content</span>
        <span class="p">}</span>
        <span class="n">news_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">article_data</span><span class="p">)</span>
</code></pre></div>

<table>
<thead>
<tr>
<th><strong>Variable</strong></th>
<th><strong>Interpretation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>title</td>
<td>The article's headline.</td>
</tr>
<tr>
<td>abstract</td>
<td>A brief summary of the article.</td>
</tr>
<tr>
<td>published_date</td>
<td>The formatted publication date.</td>
</tr>
<tr>
<td>url</td>
<td>The URL of the article.</td>
</tr>
<tr>
<td>content</td>
<td>The main text content of the article.</td>
</tr>
</tbody>
</table>
<p>Here is a presentation of the text data results from 2023-2024:  </p>
<p><img alt="Picture showing Powell" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/images/Human-Superintelligence_01_NYT-data.jpg"></p>
<h2>Web Scraping on Reddit</h2>
<h3>1. Importing PRAW and Setting Up the Reddit API</h3>
<p>We also collected text data from the Reddit forum. We selected three subreddits, namely <strong>"r/gold"</strong>, <strong>"r/investment"</strong>, and <strong>"r/personalfinance"</strong>, as the sources of the text data. In these subreddits, we used <strong>"gold price"</strong> as the search keyword to search for posts, comments, and replies on comments.</p>
<p>Before crawling text data, we need to create a Reddit account and import the <a href="https://github.com/praw-dev/praw">PRAW</a> package. Then we apply an API from <a href="https://www.reddit.com/prefs/apps">preferences (reddit.com)</a>and establish a connection with the Reddit forum.</p>
<blockquote>
<p>PRAW, an acronym for "Python Reddit API Wrapper", is a Python package that allows for simple access to Reddit's API. PRAW aims to be easy to use and internally follows all of Reddit's API rules. With PRAW there's no need to introduce sleep calls in your code. Give your client an appropriate user agent and you're set.</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">praw</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># Set up Reddit API client</span>
<span class="n">reddit</span> <span class="o">=</span> <span class="n">praw</span><span class="o">.</span><span class="n">Reddit</span><span class="p">(</span><span class="n">client_id</span><span class="o">=</span><span class="s1">&#39;client_id&#39;</span><span class="p">,</span>  
                     <span class="n">client_secret</span><span class="o">=</span><span class="s1">&#39;client_secret&#39;</span><span class="p">,</span>  
                     <span class="n">user_agent</span><span class="o">=</span><span class="s1">&#39;user_agent&#39;</span><span class="p">)</span> 
</code></pre></div>

<h3>2. Crawling Data and Variable Interpretation</h3>
<p>We have written the following function <strong>get_posts</strong>. By default, it can crawl the top 10 posts under the <strong>subreddit_name</strong> and the search keyword <strong>topic</strong>, as well as the top 20 comments of each post and the replies to each comment.</p>
<p>When crawling the data, we focused on the <strong>comments</strong> and the <strong>replies</strong> to the comments, rather than the content of the posts themselves. This is because some posts are too long, and only several sentences in them reflect the author's views and attitudes. This will lead to a large amount of crawled data, most of which are invalid texts. In contrast, the comment section has more user participation. <strong>The opinions in the comments are often more genuine, unadorned, and concise , which makes it convenient for us to collect market sentiment information with higher density and quality.</strong></p>
<p>In addition, we also pay attention to the <strong>scores</strong> of comments and replies, which are obtained by subtracting the number of downvotes from the number of upvotes for that comment or reply. A higher score often reflects that the content of the comment is of higher quality and has a higher level of community recognition. Since only a part of users will leave comments, while another part of the users just browse and upvote without making comments, <strong>comments with high scores can better represent the emotions of the community users.</strong> In our subsequent analysis of market sentiment, we can assign higher weights to comments with high scores.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_posts</span><span class="p">(</span><span class="n">subreddit_names</span><span class="p">,</span> <span class="n">topic</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">comment_limit</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">posts</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Loop through each subreddit</span>
    <span class="k">for</span> <span class="n">subreddit_name</span> <span class="ow">in</span> <span class="n">subreddit_names</span><span class="p">:</span>
        <span class="n">subreddit</span> <span class="o">=</span> <span class="n">reddit</span><span class="o">.</span><span class="n">subreddit</span><span class="p">(</span><span class="n">subreddit_name</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scraping subreddit: </span><span class="si">{</span><span class="n">subreddit_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">subreddit</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">topic</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">limit</span><span class="p">):</span>
            <span class="n">post_info</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;subreddit&#39;</span><span class="p">:</span> <span class="n">subreddit_name</span><span class="p">,</span>  
                <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">post</span><span class="o">.</span><span class="n">title</span><span class="p">,</span>
                <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="n">post</span><span class="o">.</span><span class="n">score</span><span class="p">,</span>
                <span class="s1">&#39;created_utc&#39;</span><span class="p">:</span> <span class="n">post</span><span class="o">.</span><span class="n">created_utc</span><span class="p">,</span>
                <span class="s1">&#39;comments&#39;</span><span class="p">:</span> <span class="p">[]</span>
            <span class="p">}</span>

            <span class="c1"># Load all comments (excluding nested replies)</span>
            <span class="n">post</span><span class="o">.</span><span class="n">comments</span><span class="o">.</span><span class="n">replace_more</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">comment</span> <span class="ow">in</span> <span class="n">post</span><span class="o">.</span><span class="n">comments</span><span class="p">[:</span><span class="n">comment_limit</span><span class="p">]:</span>  <span class="c1"># Keep the top comments</span>
                <span class="n">comment_info</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">comment</span><span class="o">.</span><span class="n">body</span><span class="p">,</span>
                    <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="n">comment</span><span class="o">.</span><span class="n">score</span><span class="p">,</span>
                    <span class="s1">&#39;replies&#39;</span><span class="p">:</span> <span class="p">[]</span>
                <span class="p">}</span>

                <span class="c1"># Fetch replies to the comment, sort by score, and keep top 5</span>
                <span class="n">comment</span><span class="o">.</span><span class="n">replies</span><span class="o">.</span><span class="n">replace_more</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">top_replies</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">comment</span><span class="o">.</span><span class="n">replies</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">score</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">reply</span> <span class="ow">in</span> <span class="n">top_replies</span><span class="p">:</span>
                    <span class="n">reply_info</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">reply</span><span class="o">.</span><span class="n">body</span><span class="p">,</span>
                        <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="n">reply</span><span class="o">.</span><span class="n">score</span>
                    <span class="p">}</span>
                    <span class="n">comment_info</span><span class="p">[</span><span class="s1">&#39;replies&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reply_info</span><span class="p">)</span>

                <span class="n">post_info</span><span class="p">[</span><span class="s1">&#39;comments&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">comment_info</span><span class="p">)</span>

            <span class="n">posts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">post_info</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">posts</span>
</code></pre></div>

<p>We selected three subreddits, namely <strong>"r/gold"</strong>, <strong>"r/investment"</strong>, and <strong>"r/personalfinance"</strong>, as the sources of text data. This is because these three subreddits have more followers, and when using <strong>"gold price"</strong> as the search keyword, a large number of posts and replies can be retrieved.</p>
<p><img alt="Picture showing Powell" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/images/Human-Superintelligence_01_subreddit.jpg"></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Set search keywords</span>
<span class="n">subreddit_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gold&#39;</span><span class="p">,</span><span class="s1">&#39;investing&#39;</span><span class="p">,</span> <span class="s1">&#39;personalfinance&#39;</span><span class="p">]</span>  <span class="c1"># List of subreddits to scrape</span>
<span class="n">topic</span> <span class="o">=</span> <span class="s1">&#39;gold price&#39;</span>
<span class="n">posts</span> <span class="o">=</span> <span class="n">get_posts</span><span class="p">(</span><span class="n">subreddit_names</span><span class="p">,</span> <span class="n">topic</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">comment_limit</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div>

<p>Here shows the results of the text data from Reddit:   </p>
<p><img alt="Picture showing Powell" src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/images/Human-Superintelligence_01_Reddit-Data.jpg"></p>
<p>The interpretations of each variable in the text data are as follows.</p>
<table>
<thead>
<tr>
<th><strong>Variable</strong></th>
<th><strong>Interpretation</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Subreddit</td>
<td>The name of the subreddit forum.</td>
</tr>
<tr>
<td>Post Title</td>
<td>The title of the post.</td>
</tr>
<tr>
<td>Post Time</td>
<td>The specific moment when a post is actually published on the Reddit.</td>
</tr>
<tr>
<td>Comment Body</td>
<td>The actual textual content of a comment in response to a post.</td>
</tr>
<tr>
<td>Comment Score</td>
<td>The net popularity or approval level of a comment.</td>
</tr>
<tr>
<td>Reply Body</td>
<td>The actual textual content of a reply in response to a comment.</td>
</tr>
<tr>
<td>Reply Score</td>
<td>The net popularity or approval level of a reply.</td>
</tr>
</tbody>
</table>
<p>The following is the code for storing the  text data.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Prepare comments data for DataFrame</span>
<span class="n">comments_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="s1">&#39;subreddit&#39;</span><span class="p">],</span>
     <span class="n">post</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">],</span> 
     <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">localtime</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="s1">&#39;created_utc&#39;</span><span class="p">])),</span> 
     <span class="n">comment</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">],</span> 
     <span class="n">comment</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">],</span>
     <span class="n">reply</span><span class="p">[</span><span class="s1">&#39;body&#39;</span><span class="p">],</span>  
     <span class="n">reply</span><span class="p">[</span><span class="s1">&#39;score&#39;</span><span class="p">]</span> 
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">posts</span> 
    <span class="k">for</span> <span class="n">comment</span> <span class="ow">in</span> <span class="n">post</span><span class="p">[</span><span class="s1">&#39;comments&#39;</span><span class="p">]</span> 
    <span class="k">for</span> <span class="n">reply</span> <span class="ow">in</span> <span class="n">comment</span><span class="p">[</span><span class="s1">&#39;replies&#39;</span><span class="p">]</span>  
<span class="p">]</span>

<span class="c1"># Create DataFrame</span>
<span class="n">df_comments</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">comments_data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span>
    <span class="s1">&#39;Subreddit&#39;</span><span class="p">,</span>  
    <span class="s1">&#39;Post Title&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;Post Time&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;Comment Body&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;Comment Score&#39;</span><span class="p">,</span> 
    <span class="s1">&#39;Reply Body&#39;</span><span class="p">,</span>  
    <span class="s1">&#39;Reply Score&#39;</span>  
<span class="p">])</span>

<span class="c1"># Display the DataFrame</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_comments</span><span class="p">)</span>

<span class="c1"># Save to CSV file (optional)</span>
<span class="n">df_comments</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;reddit_comments_with_replies.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
</code></pre></div>

<h2>Problems We Solved</h2>
<h3>1. Choosing Between NYT Article API and Google News</h3>
<p>In the process of building a gold market sentiment analysis system, crawling news data is a crucial step. Initially, we faced two main choices: <strong>NYT Article API</strong> and <strong>Google News</strong>. After in-depth analysis and practical testing, we ultimately chose the NYT Article API as our primary data source. Below is our thought process and rationale for this decision.</p>
<p>Our goal was to crawl gold-related news data spanning from <strong>2014 to 2024</strong>, a period of ten years. This extensive time frame meant the data volume would be massive and complex. If we chose Google News as the data source, despite its wide coverage, its search results are often too vast and fragmented, making it highly prone to triggering <strong>429 errors (Too Many Requests)</strong>. Such errors not only interrupt the crawling process but also add extra processing costs and time consumption.</p>
<p>In contrast, the NYT Article API provides a more centralized data source. As one of the world's most authoritative media outlets, The New York Times offers news coverage with high credibility and representativeness, better reflecting the sentiment changes in the gold market. Additionally, the API's design allows us to effectively avoid 429 errors by rationally planning request frequencies and crawling in batches.</p>
<p>The quality of news data directly impacts the accuracy of subsequent analysis. While Google News has extensive coverage, its search results include a large amount of news from unverified sources and duplicate content, making data cleaning and filtering a significant challenge. On the other hand, the NYT Article API provides data from a single, authoritative source, ensuring the reliability and consistency of news content. This is particularly important for our analysis of gold market sentiment, as reports from authoritative media often better reflect the true sentiment and trends of the market.</p>
<h3>2. Error Handling</h3>
<p>The inclusion of <strong>Error Handling</strong> is aimed at enhancing the program's robustness and reliability, ensuring that the program does not crash when encountering issues such as failed network requests, data parsing exceptions, or other unexpected errors. Instead, it captures errors and provides clear prompts. By addressing exceptions such as network issues, API limits, date format errors, and article retrieval failures, the error handling mechanism ensures the program can operate stably in complex environments, while also improving user experience and code maintainability.</p>
<div class="highlight"><pre><span></span><code><span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">exceptions</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Network request error:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Other error:&quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</code></pre></div>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2025-01-17T01:12:00+08:00">Fri 17 January 2025</time>
            <h4>Category</h4>
            <a class="category-link" href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/categories.html#progress-report-ref">Progress Report</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/tags.html#group-human-superintelligence-ref">Group Human Superintelligence
                    <span class="superscript">1</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/buehlmaier/MFIN7036-student-blog-2025-01" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://buehlmaier.github.io/MFIN7036-student-blog-2025-01/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>